# AI æ¨¡å‹æŒ‡å—

> ğŸ§  Make10 å°ˆæ¡ˆçš„ AI æ¨¡å‹æ¶æ§‹ã€è¨“ç·´æµç¨‹èˆ‡éƒ¨ç½²ç­–ç•¥ (é–‹ç™¼ä¸­)

## ğŸš§ é–‹ç™¼ç‹€æ…‹

### ç›®å‰é€²åº¦
- âœ… **æŠ€è¡“æ¶æ§‹è¦åŠƒ** - å®Œæˆæ¨¡å‹è¨­è¨ˆèˆ‡æŠ€è¡“é¸å‹
- âœ… **ç’°å¢ƒé…ç½®** - TensorFlow 2.19.0 å·²å®‰è£é…ç½®
- âœ… **è³‡æ–™çµæ§‹** - è¨“ç·´è³‡æ–™ç›®éŒ„èˆ‡æ¨¡å‹å­˜å„²è·¯å¾‘å·²è¨­å®š
- ğŸš§ **æ¨¡å‹å¯¦ä½œ** - å¾…é–‹ç™¼ (`src/ai/` æ¨¡çµ„)
- ğŸš§ **è¨“ç·´æµç¨‹** - å¾…é–‹ç™¼ (`run_training.py`)
- ğŸš§ **æ¨™ç±¤å·¥å…·** - å¾…é–‹ç™¼ (`src/labeling/`)

### è¦åŠƒé€²åº¦
```mermaid
gantt
    title AI æ¨¡å‹é–‹ç™¼é€²åº¦
    dateFormat  YYYY-MM-DD
    section åŸºç¤è¨­æ–½
    æŠ€è¡“æ¶æ§‹è¦åŠƒ    :done, arch, 2024-01-01, 2024-01-15
    ç’°å¢ƒé…ç½®       :done, env, 2024-01-10, 2024-01-20
    section æ¨¡å‹é–‹ç™¼
    CNN æ¨¡å‹å¯¦ä½œ   :active, model, 2024-01-20, 2024-02-05
    è¨“ç·´æµç¨‹é–‹ç™¼   :training, after model, 15d
    section è³‡æ–™è™•ç†
    æ¨™ç±¤å·¥å…·é–‹ç™¼   :labeling, 2024-02-01, 2024-02-15
    è³‡æ–™æ“´å¢      :augment, after labeling, 10d
```

## ğŸ¯ AI æ¨¡å‹è¦åŠƒ

### æ¨¡å‹ç›®æ¨™
- **ä¸»è¦ä»»å‹™**: æ•¸å­—è­˜åˆ¥ (0-9 åé¡åˆ¥åˆ†é¡)
- **è¼¸å…¥æ ¼å¼**: 224Ã—224 RGB å½±åƒ (ResNet50 æ¨™æº–è¼¸å…¥)
- **è¼¸å‡ºæ ¼å¼**: 10 é¡åˆ¥æ©Ÿç‡åˆ†ä½ˆ
- **ç›®æ¨™æº–ç¢ºç‡**: >98% (é©—è­‰é›†ï¼ŒResNet50 æ·±åº¦ç¶²è·¯)
- **æ¨ç†é€Ÿåº¦**: <10ms (å–®æ¬¡é æ¸¬)
- **æ¨¡å‹å¤§å°**: <100MB (ResNet50 é è¨“ç·´æ¨¡å‹)

### æŠ€è¡“æ£§é¸æ“‡
```python
# æ ¸å¿ƒæ¡†æ¶
tensorflow = "2.19.0"        # æ·±åº¦å­¸ç¿’æ¡†æ¶
numpy = "2.1.3"             # æ•¸å€¼è¨ˆç®—
opencv-python = "4.12.0.88" # å½±åƒé è™•ç†

# è³‡æ–™è™•ç†
pandas = "2.3.1"            # è³‡æ–™åˆ†æ
pillow = "11.3.0"           # å½±åƒæ ¼å¼æ”¯æ´
```

## ğŸ—ï¸ æ¨¡å‹æ¶æ§‹è¨­è¨ˆ

### ResNet50 ç¶²è·¯çµæ§‹ (æ›´æ–°)
```python
def create_resnet50_digit_model(pretrained=True):
    """å»ºç«‹åŸºæ–¼ ResNet50 çš„æ•¸å­—è­˜åˆ¥æ¨¡å‹"""
    
    # è¼‰å…¥é è¨“ç·´ ResNet50 (ä¸å«é ‚å±¤)
    base_model = tf.keras.applications.ResNet50(
        weights='imagenet' if pretrained else None,
        include_top=False,
        input_shape=(224, 224, 3)
    )
    
    # å‡çµé è¨“ç·´å±¤ (å¯é¸)
    if pretrained:
        base_model.trainable = False
    
    # æ·»åŠ è‡ªå®šç¾©åˆ†é¡å±¤
    model = tf.keras.Sequential([
        # è¼¸å…¥å±¤
        tf.keras.layers.Input(shape=(224, 224, 3)),
        
        # ResNet50 ç‰¹å¾µæå–
        base_model,
        
        # å…¨åŸŸå¹³å‡æ± åŒ–
        tf.keras.layers.GlobalAveragePooling2D(),
        
        # å…¨é€£æ¥å±¤
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.BatchNormalization(),
        
        # è¼¸å‡ºå±¤ (10 é¡åˆ¥æ•¸å­—åˆ†é¡)
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    return model

def create_resnet50_from_scratch():
    """å¾é ­è¨“ç·´ ResNet50 (é©ç”¨æ–¼å¤§é‡è³‡æ–™)"""
    model = tf.keras.applications.ResNet50(
        weights=None,  # ä¸ä½¿ç”¨é è¨“ç·´æ¬Šé‡
        include_top=False,
        input_shape=(224, 224, 3)
    )
    
    # æ·»åŠ åˆ†é¡é ­
    x = tf.keras.layers.GlobalAveragePooling2D()(model.output)
    x = tf.keras.layers.Dense(512, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.5)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    outputs = tf.keras.layers.Dense(10, activation='softmax')(x)
    
    final_model = tf.keras.Model(inputs=model.input, outputs=outputs)
    return final_model
```

### é·ç§»å­¸ç¿’ç­–ç•¥
```python
def fine_tune_resnet50(model, train_data, val_data):
    """ResNet50 é·ç§»å­¸ç¿’å¾®èª¿"""
    
    # éšæ®µ 1: å‡çµé è¨“ç·´å±¤ï¼Œåªè¨“ç·´åˆ†é¡é ­
    model.layers[1].trainable = False  # base_model å‡çµ
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # è¨“ç·´åˆ†é¡é ­
    history_1 = model.fit(
        train_data,
        validation_data=val_data,
        epochs=10,
        verbose=1
    )
    
    # éšæ®µ 2: è§£å‡éƒ¨åˆ†å±¤é€²è¡Œå¾®èª¿
    model.layers[1].trainable = True  # è§£å‡ base_model
    
    # å‡çµå‰é¢çš„å±¤ï¼Œåªå¾®èª¿å¾Œé¢çš„å±¤
    for layer in model.layers[1].layers[:-20]:
        layer.trainable = False
    
    # ä½¿ç”¨è¼ƒå°çš„å­¸ç¿’ç‡å¾®èª¿
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # ç¹¼çºŒè¨“ç·´
    history_2 = model.fit(
        train_data,
        validation_data=val_data,
        epochs=20,
        verbose=1
    )
    
    return model, history_1, history_2
```

### æ¨¡å‹é…ç½®åƒæ•¸
```python
# config/settings.py - AI æ¨¡å‹é…ç½® (ResNet50)
cfg.MODEL = EasyDict({
    'input_shape': (224, 224, 3),        # ResNet50 æ¨™æº–è¼¸å…¥å°ºå¯¸
    'num_classes': 10,                   # åˆ†é¡æ•¸é‡ (0-9)
    'batch_size': 16,                    # æ‰¹æ¬¡å¤§å° (ResNet50 è¨˜æ†¶é«”éœ€æ±‚è¼ƒå¤§)
    'confidence_threshold': 0.9,         # ä¿¡å¿ƒåº¦é–¾å€¼ (æé«˜ç²¾åº¦)
    'voting_threshold': 0.7,             # æŠ•ç¥¨é–¾å€¼
    'use_pretrained': True,              # ä½¿ç”¨é è¨“ç·´æ¬Šé‡
    'fine_tune_layers': 20,              # å¾®èª¿çš„å±¤æ•¸
})

cfg.TRAINING = EasyDict({
    'epochs_stage1': 10,                 # ç¬¬ä¸€éšæ®µè¨“ç·´è¼ªæ•¸ (å‡çµé è¨“ç·´å±¤)
    'epochs_stage2': 20,                 # ç¬¬äºŒéšæ®µè¨“ç·´è¼ªæ•¸ (å¾®èª¿)
    'learning_rate_stage1': 0.001,       # ç¬¬ä¸€éšæ®µå­¸ç¿’ç‡
    'learning_rate_stage2': 0.0001,      # ç¬¬äºŒéšæ®µå­¸ç¿’ç‡ (è¼ƒå°)
    'validation_split': 0.2,             # é©—è­‰é›†æ¯”ä¾‹
    'early_stopping_patience': 15,       # æ—©åœå¿è€åº¦ (å¢åŠ )
})

cfg.DATA_AUGMENTATION = EasyDict({
    'rotation_range': 10,                # æ—‹è½‰è§’åº¦ç¯„åœ
    'width_shift_range': 0.1,            # æ°´å¹³ç§»å‹•ç¯„åœ
    'height_shift_range': 0.1,           # å‚ç›´ç§»å‹•ç¯„åœ
    'zoom_range': 0.1,                   # ç¸®æ”¾ç¯„åœ
    'horizontal_flip': False,            # ä¸æ°´å¹³ç¿»è½‰ (æ•¸å­—)
    'fill_mode': 'constant',             # å¡«å……æ¨¡å¼
    'cval': 0.0,                         # å¡«å……å€¼
})
```

## ğŸ“Š è³‡æ–™æµç¨‹è¨­è¨ˆ

### è³‡æ–™çµæ§‹ (ç›®å‰å·²å»ºç«‹)
```
data/
â”œâ”€â”€ training/                # è¨“ç·´è³‡æ–™é›†
â”‚   â”œâ”€â”€ images/             # è¨“ç·´å½±åƒæª”æ¡ˆ
â”‚   â””â”€â”€ labels/             # å°æ‡‰æ¨™ç±¤è³‡æ–™
â”œâ”€â”€ models/                 # æ¨¡å‹å­˜å„²
â”‚   â”œâ”€â”€ checkpoints/        # è¨“ç·´æª¢æŸ¥é»
â”‚   â””â”€â”€ exports/            # æœ€çµ‚åŒ¯å‡ºæ¨¡å‹
â”‚       â””â”€â”€ model.keras     # ä¸»è¦æ¨¡å‹æª”æ¡ˆ
â””â”€â”€ assets/                 # è¼”åŠ©è³‡æº
    â””â”€â”€ templates/          # æ¨¡æ¿å½±åƒ
```

### è³‡æ–™é è™•ç†æµç¨‹ (ResNet50 é©é…)
```python
def preprocess_image_for_resnet50(image):
    """å½±åƒé è™•ç†ç®¡é“ - ResNet50 ç‰ˆæœ¬"""
    
    # 1. å°ºå¯¸èª¿æ•´åˆ° ResNet50 æ¨™æº–è¼¸å…¥
    resized = cv2.resize(image, (224, 224))
    
    # 2. ç°éšè½‰ RGB (å¦‚æœéœ€è¦)
    if len(resized.shape) == 2:
        # ç°éšå½±åƒè½‰ç‚º 3 é€šé“
        rgb_image = cv2.cvtColor(resized, cv2.COLOR_GRAY2RGB)
    elif resized.shape[2] == 1:
        rgb_image = np.repeat(resized, 3, axis=2)
    else:
        rgb_image = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
    
    # 3. æ­£è¦åŒ– (ImageNet é è¨“ç·´æ¨™æº–)
    normalized = rgb_image.astype(np.float32)
    normalized = tf.keras.applications.resnet50.preprocess_input(normalized)
    
    # 4. æ–°å¢æ‰¹æ¬¡ç¶­åº¦
    batched = np.expand_dims(normalized, axis=0)
    
    return batched

def augment_training_data_resnet50(images, labels):
    """ResNet50 å°ˆç”¨è³‡æ–™æ“´å¢ç­–ç•¥"""
    
    datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        rotation_range=10,           # è¼•å¾®æ—‹è½‰
        width_shift_range=0.1,       # æ°´å¹³ç§»å‹•
        height_shift_range=0.1,      # å‚ç›´ç§»å‹•
        zoom_range=0.1,              # ç¸®æ”¾
        brightness_range=[0.8, 1.2], # äº®åº¦èª¿æ•´
        fill_mode='constant',        # é‚Šç•Œå¡«å……
        cval=0.0,                    # å¡«å……å€¼
        preprocessing_function=tf.keras.applications.resnet50.preprocess_input
    )
    
    # ç”Ÿæˆæ“´å¢è³‡æ–™
    augmented_generator = datagen.flow(
        images, labels,
        batch_size=32,
        shuffle=True
    )
    
    return augmented_generator

def create_data_pipeline_resnet50(image_dir, label_file, batch_size=16):
    """å»ºç«‹ ResNet50 è³‡æ–™ç®¡é“"""
    
    # è®€å–è³‡æ–™
    images, labels = load_training_data(image_dir, label_file)
    
    # è½‰æ›ç‚º TensorFlow è³‡æ–™é›†
    dataset = tf.data.Dataset.from_tensor_slices((images, labels))
    
    # é è™•ç†å‡½å¼
    def preprocess_fn(image, label):
        # èª¿æ•´å¤§å°ä¸¦è½‰æ›ç‚º RGB
        image = tf.image.resize(image, [224, 224])
        image = tf.cast(image, tf.float32)
        
        # å¦‚æœæ˜¯ç°éšï¼Œè¤‡è£½åˆ° 3 å€‹é€šé“
        if tf.shape(image)[-1] == 1:
            image = tf.repeat(image, 3, axis=-1)
        
        # ImageNet æ¨™æº–åŒ–
        image = tf.keras.applications.resnet50.preprocess_input(image)
        
        return image, label
    
    # æ‡‰ç”¨é è™•ç†
    dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)
    
    # æ‰¹æ¬¡åŒ–å’Œå¿«å–
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
    return dataset
```

### æ¨ç†æµç¨‹ (ResNet50 ç‰ˆæœ¬)
```python
class ResNet50DigitPredictor:
    """åŸºæ–¼ ResNet50 çš„æ•¸å­—è­˜åˆ¥é æ¸¬å™¨"""
    
    def __init__(self, model_path):
        self.model = tf.keras.models.load_model(model_path)
        self.input_size = (224, 224)
        
    def preprocess_single_image(self, image_region):
        """å–®ä¸€å½±åƒé è™•ç†"""
        # èª¿æ•´å¤§å°
        resized = cv2.resize(image_region, self.input_size)
        
        # ç¢ºä¿ RGB æ ¼å¼
        if len(resized.shape) == 2:
            rgb_image = cv2.cvtColor(resized, cv2.COLOR_GRAY2RGB)
        elif resized.shape[2] == 1:
            rgb_image = np.repeat(resized, 3, axis=2)
        else:
            rgb_image = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
        
        # ImageNet æ¨™æº–é è™•ç†
        processed = tf.keras.applications.resnet50.preprocess_input(
            rgb_image.astype(np.float32)
        )
        
        # æ–°å¢æ‰¹æ¬¡ç¶­åº¦
        return np.expand_dims(processed, axis=0)
        
    def predict_digit(self, image_region):
        """é æ¸¬å–®ä¸€æ•¸å­—"""
        # é è™•ç†
        processed = self.preprocess_single_image(image_region)
        
        # æ¨¡å‹æ¨ç†
        prediction = self.model.predict(processed, verbose=0)
        
        # çµæœè§£æ
        digit = np.argmax(prediction[0])
        confidence = np.max(prediction[0])
        
        # æ‰€æœ‰é¡åˆ¥çš„æ©Ÿç‡
        probabilities = prediction[0]
        
        return {
            'digit': int(digit),
            'confidence': float(confidence),
            'probabilities': probabilities.tolist()
        }
    
    def predict_batch(self, image_regions):
        """æ‰¹æ¬¡é æ¸¬å¤šå€‹æ•¸å­—"""
        # æ‰¹æ¬¡é è™•ç†
        batch_data = []
        for img in image_regions:
            processed = self.preprocess_single_image(img)
            batch_data.append(processed[0])  # ç§»é™¤æ‰¹æ¬¡ç¶­åº¦
        
        batch = np.stack(batch_data)
        
        # æ‰¹æ¬¡æ¨ç†
        predictions = self.model.predict(batch, verbose=0)
        
        # è§£æçµæœ
        results = []
        for pred in predictions:
            digit = np.argmax(pred)
            confidence = np.max(pred)
            results.append({
                'digit': int(digit),
                'confidence': float(confidence),
                'probabilities': pred.tolist()
            })
            
        return results
    
    def predict_with_tta(self, image_region, num_augmentations=5):
        """ä½¿ç”¨æ¸¬è©¦æ™‚å¢å¼· (TTA) æé«˜é æ¸¬æº–ç¢ºæ€§"""
        predictions = []
        
        # åŸå§‹é æ¸¬
        original_pred = self.predict_digit(image_region)
        predictions.append(original_pred['probabilities'])
        
        # å¢å¼·ç‰ˆæœ¬é æ¸¬
        for _ in range(num_augmentations):
            # è¼•å¾®æ—‹è½‰å’Œç¸®æ”¾
            h, w = image_region.shape[:2]
            angle = np.random.uniform(-5, 5)
            scale = np.random.uniform(0.95, 1.05)
            
            center = (w//2, h//2)
            M = cv2.getRotationMatrix2D(center, angle, scale)
            augmented = cv2.warpAffine(image_region, M, (w, h))
            
            pred = self.predict_digit(augmented)
            predictions.append(pred['probabilities'])
        
        # å¹³å‡é æ¸¬çµæœ
        avg_probs = np.mean(predictions, axis=0)
        final_digit = np.argmax(avg_probs)
        final_confidence = np.max(avg_probs)
        
        return {
            'digit': int(final_digit),
            'confidence': float(final_confidence),
            'probabilities': avg_probs.tolist(),
            'tta_predictions': len(predictions)
        }
```

## ğŸƒâ€â™‚ï¸ è¨“ç·´æµç¨‹è¨­è¨ˆ

### è¨“ç·´ç®¡é“ (è¦åŠƒ)
```python
# run_training.py - ä¸»è¦è¨“ç·´å…¥å£
def main_training_pipeline():
    """ä¸»è¦è¨“ç·´æµç¨‹"""
    
    # 1. è³‡æ–™è¼‰å…¥èˆ‡é è™•ç†
    train_data, val_data = load_training_data()
    
    # 2. æ¨¡å‹å»ºç«‹èˆ‡ç·¨è­¯
    model = create_digit_recognition_model()
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # 3. è¨“ç·´é…ç½®
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            'data/models/checkpoints/best_model.h5',
            save_best_only=True
        ),
        tf.keras.callbacks.EarlyStopping(
            patience=10,
            restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            factor=0.5,
            patience=5
        )
    ]
    
    # 4. åŸ·è¡Œè¨“ç·´
    history = model.fit(
        train_data,
        validation_data=val_data,
        epochs=50,
        callbacks=callbacks
    )
    
    # 5. æ¨¡å‹è©•ä¼°èˆ‡å„²å­˜
    evaluate_and_save_model(model, val_data)
    
    return model, history
```

### æ¨¡å‹è©•ä¼°ç­–ç•¥ (è¦åŠƒ)
```python
def evaluate_model_performance(model, test_data):
    """æ¨¡å‹æ•ˆèƒ½è©•ä¼°"""
    
    # åŸºæœ¬æº–ç¢ºç‡
    test_loss, test_accuracy = model.evaluate(test_data)
    
    # æ··æ·†çŸ©é™£
    predictions = model.predict(test_data)
    y_pred = np.argmax(predictions, axis=1)
    y_true = # ... å¾ test_data å–å¾—çœŸå¯¦æ¨™ç±¤
    
    confusion_matrix = tf.math.confusion_matrix(y_true, y_pred)
    
    # å„é¡åˆ¥æº–ç¢ºç‡
    class_accuracy = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=1)
    
    # æ¨ç†é€Ÿåº¦æ¸¬è©¦
    inference_times = benchmark_inference_speed(model)
    
    return {
        'accuracy': test_accuracy,
        'class_accuracy': class_accuracy,
        'inference_time': np.mean(inference_times)
    }
```

## ğŸš€ éƒ¨ç½²ç­–ç•¥

### æ¨¡å‹æœ€ä½³åŒ– (è¦åŠƒ)
```python
def optimize_model_for_deployment(model):
    """æ¨¡å‹éƒ¨ç½²æœ€ä½³åŒ–"""
    
    # é‡åŒ–è™•ç†
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    
    # è½‰æ›ç‚º TensorFlow Lite
    tflite_model = converter.convert()
    
    # å„²å­˜æœ€ä½³åŒ–æ¨¡å‹
    with open('data/models/exports/model_optimized.tflite', 'wb') as f:
        f.write(tflite_model)
    
    return tflite_model

def benchmark_deployment_performance():
    """éƒ¨ç½²æ•ˆèƒ½æ¸¬è©¦"""
    # è¨˜æ†¶é«”ä½¿ç”¨é‡
    # æ¨ç†é€Ÿåº¦
    # æº–ç¢ºç‡æå¤±
    pass
```

### æ•´åˆç­–ç•¥
```python
# åœ¨ src/automation/screen_utils.py ä¸­æ•´åˆ
from src.ai.predictor import DigitPredictor

class GameAutomationSystem:
    def __init__(self):
        # è¼‰å…¥ AI æ¨¡å‹
        self.predictor = DigitPredictor(cfg.PATHS.MODEL.main_model)
    
    def recognize_game_numbers(self, screenshot):
        """è­˜åˆ¥éŠæˆ²ä¸­çš„æ•¸å­—"""
        # 1. æå–æ•¸å­—å€åŸŸ
        number_regions = extract_number_regions(screenshot)
        
        # 2. AI è­˜åˆ¥
        digits = self.predictor.predict_batch(number_regions)
        
        # 3. ä¿¡å¿ƒåº¦éæ¿¾
        reliable_digits = [
            (digit, pos) for (digit, conf), pos 
            in zip(digits, number_regions)
            if conf > cfg.MODEL.confidence_threshold
        ]
        
        return reliable_digits
```

## ğŸ“ˆ æ•ˆèƒ½æŒ‡æ¨™èˆ‡ç›£æ§

### é—œéµæŒ‡æ¨™ (è¦åŠƒ)
- **æº–ç¢ºç‡**: >95% (æ•´é«”æº–ç¢ºç‡)
- **å„é¡åˆ¥æº–ç¢ºç‡**: >90% (æ¯å€‹æ•¸å­—é¡åˆ¥)
- **æ¨ç†é€Ÿåº¦**: <5ms (å–®æ¬¡é æ¸¬)
- **è¨˜æ†¶é«”ä½¿ç”¨**: <100MB (æ¨¡å‹è¼‰å…¥å¾Œ)
- **æ¨¡å‹å¤§å°**: <10MB (å„²å­˜æª”æ¡ˆ)

### ç›£æ§æ©Ÿåˆ¶ (è¦åŠƒ)
```python
def log_prediction_metrics(predictions, ground_truth):
    """è¨˜éŒ„é æ¸¬æ•ˆèƒ½æŒ‡æ¨™"""
    accuracy = calculate_accuracy(predictions, ground_truth)
    confidence_dist = analyze_confidence_distribution(predictions)
    
    logger.info(f"é æ¸¬æº–ç¢ºç‡: {accuracy:.3f}")
    logger.info(f"å¹³å‡ä¿¡å¿ƒåº¦: {confidence_dist['mean']:.3f}")
    
def monitor_inference_performance():
    """ç›£æ§æ¨ç†æ•ˆèƒ½"""
    start_time = time.time()
    # ... åŸ·è¡Œæ¨ç†
    inference_time = time.time() - start_time
    
    if inference_time > 0.01:  # 10ms é–¾å€¼
        logger.warning(f"æ¨ç†æ™‚é–“éé•·: {inference_time:.3f}s")
```

## ğŸ”— å¾ŒçºŒé–‹ç™¼è¨ˆç•«

### çŸ­æœŸç›®æ¨™ (1-2 å€‹æœˆ)
- [ ] å¯¦ä½œ `src/ai/model_manager.py` - æ¨¡å‹ç®¡ç†å™¨
- [ ] å¯¦ä½œ `src/ai/predictor.py` - é æ¸¬å™¨æ¨¡çµ„
- [ ] å®Œæˆ `run_training.py` - è¨“ç·´æµç¨‹
- [ ] å»ºç«‹åŸºç¤è³‡æ–™é›†èˆ‡æ¨™ç±¤

### ä¸­æœŸç›®æ¨™ (3-6 å€‹æœˆ)
- [ ] é–‹ç™¼ `src/labeling/` - è³‡æ–™æ¨™è¨»å·¥å…·
- [ ] æ¨¡å‹æ•ˆèƒ½æœ€ä½³åŒ–èˆ‡é‡åŒ–
- [ ] å»ºç«‹æ¨¡å‹ç‰ˆæœ¬ç®¡ç†ç³»çµ±
- [ ] è‡ªå‹•åŒ–è¨“ç·´èˆ‡è©•ä¼°æµç¨‹

### é•·æœŸç›®æ¨™ (6+ å€‹æœˆ)
- [ ] å¤šæ¨¡å‹é›†æˆèˆ‡æŠ•ç¥¨æ©Ÿåˆ¶
- [ ] ç·šä¸Šå­¸ç¿’èˆ‡æ¨¡å‹æ›´æ–°
- [ ] A/B æ¸¬è©¦æ¡†æ¶
- [ ] é›²ç«¯è¨“ç·´èˆ‡éƒ¨ç½²

### æ¨¡å‹åƒæ•¸èªªæ˜
| å±¤ç´š | é¡å‹ | è¼¸å‡ºå½¢ç‹€ | åƒæ•¸æ•¸é‡ | èªªæ˜ |
|------|------|----------|----------|------|
| Input | InputLayer | (None, 28, 28, 1) | 0 | è¼¸å…¥å±¤ |
| Conv2D_1 | Conv2D | (None, 28, 28, 16) | 160 | 3x3 å·ç©ï¼Œ16 æ¿¾æ³¢å™¨ |
| MaxPool_1 | MaxPooling2D | (None, 14, 14, 16) | 0 | 2x2 æœ€å¤§æ± åŒ– |
| Dropout_1 | Dropout | (None, 14, 14, 16) | 0 | 20% ä¸Ÿæ£„ç‡ |
| Conv2D_2 | Conv2D | (None, 14, 14, 32) | 4,640 | 3x3 å·ç©ï¼Œ32 æ¿¾æ³¢å™¨ |
| MaxPool_2 | MaxPooling2D | (None, 7, 7, 32) | 0 | 2x2 æœ€å¤§æ± åŒ– |
| Dropout_2 | Dropout | (None, 7, 7, 32) | 0 | 30% ä¸Ÿæ£„ç‡ |
| Flatten | Flatten | (None, 1568) | 0 | æ‹‰å¹³ |
| Dense_1 | Dense | (None, 64) | 100,416 | å…¨é€£æ¥å±¤ |
| Dropout_3 | Dropout | (None, 64) | 0 | 50% ä¸Ÿæ£„ç‡ |
| Dense_2 | Dense | (None, 10) | 650 | è¼¸å‡ºå±¤ |

**ç¸½åƒæ•¸**: 105,866 å€‹

### æ¨¡å‹ç·¨è­¯è¨­å®š
```python
model.compile(
    optimizer='adam',               # Adam å„ªåŒ–å™¨
    loss='categorical_crossentropy', # åˆ†é¡äº¤å‰ç†µæå¤±
    metrics=['accuracy']            # æº–ç¢ºç‡è©•ä¼°
)
```

## ğŸ“Š è³‡æ–™è™•ç†æµç¨‹

### åœ–åƒé è™•ç†ç®¡é“
```python
def preprocess_cell_image(cell_image):
    """é è™•ç† cell åœ–åƒ"""
    # 1. å°ºå¯¸èª¿æ•´
    resized = cv2.resize(cell_image, (28, 28))
    
    # 2. ç°éšè½‰æ›
    if len(resized.shape) == 3:
        gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)
    else:
        gray = resized
    
    # 3. æ­£è¦åŒ– (0-1)
    normalized = gray.astype(np.float32) / 255.0
    
    # 4. å¢åŠ æ‰¹æ¬¡ç¶­åº¦
    batch_ready = normalized.reshape(1, 28, 28, 1)
    
    return batch_ready
```

### è³‡æ–™å¢å¼·æŠ€è¡“
```python
def augment_training_data(images, labels):
    """è³‡æ–™å¢å¼·ä»¥æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›"""
    augmented_images = []
    augmented_labels = []
    
    for img, label in zip(images, labels):
        # åŸå§‹åœ–åƒ
        augmented_images.append(img)
        augmented_labels.append(label)
        
        # æ—‹è½‰ (-15 åˆ° 15 åº¦)
        angle = random.uniform(-15, 15)
        rotated = rotate_image(img, angle)
        augmented_images.append(rotated)
        augmented_labels.append(label)
        
        # ç¸®æ”¾ (0.9 åˆ° 1.1 å€)
        scale = random.uniform(0.9, 1.1)
        scaled = scale_image(img, scale)
        augmented_images.append(scaled)
        augmented_labels.append(label)
        
        # å¹³ç§» (-2 åˆ° 2 åƒç´ )
        dx, dy = random.randint(-2, 2), random.randint(-2, 2)
        translated = translate_image(img, dx, dy)
        augmented_images.append(translated)
        augmented_labels.append(label)
    
    return np.array(augmented_images), np.array(augmented_labels)
```

## ğŸ“ è¨“ç·´å·¥ä½œæµç¨‹

### å®Œæ•´è¨“ç·´ç®¡é“
```python
def train_model(force_new=False, use_mnist=False):
    """å®Œæ•´çš„æ¨¡å‹è¨“ç·´ç®¡é“"""
    
    # 1. è³‡æ–™è¼‰å…¥
    if use_mnist:
        X_train, y_train, X_test, y_test = load_mnist_data()
    else:
        X_train, y_train, X_test, y_test = load_custom_data()
    
    # 2. è³‡æ–™é è™•ç†
    X_train = preprocess_training_data(X_train)
    X_test = preprocess_training_data(X_test)
    
    # 3. æ¨™ç±¤ç·¨ç¢¼
    y_train = to_categorical(y_train, 10)
    y_test = to_categorical(y_test, 10)
    
    # 4. è³‡æ–™å¢å¼·
    X_train_aug, y_train_aug = augment_training_data(X_train, y_train)
    
    # 5. æ¨¡å‹å»ºç«‹æˆ–è¼‰å…¥
    if force_new or not model_exists():
        model = create_model()
    else:
        model = load_existing_model()
    
    # 6. è¨“ç·´è¨­å®š
    callbacks = [
        EarlyStopping(
            monitor='val_accuracy',
            patience=5,
            restore_best_weights=True
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            min_lr=1e-6
        ),
        ModelCheckpoint(
            filepath='checkpoints/model_epoch_{epoch:02d}.keras',
            save_best_only=True,
            monitor='val_accuracy'
        )
    ]
    
    # 7. æ¨¡å‹è¨“ç·´
    history = model.fit(
        X_train_aug, y_train_aug,
        validation_data=(X_test, y_test),
        epochs=20,
        batch_size=16,
        callbacks=callbacks,
        verbose=1
    )
    
    # 8. æ¨¡å‹è©•ä¼°
    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
    
    # 9. æ¨¡å‹å„²å­˜
    save_model(model, f"model_acc_{test_accuracy:.3f}.keras")
    
    return model, history, test_accuracy
```

### è¨“ç·´åƒæ•¸èª¿æ•´
```python
# åŸºç¤è¨“ç·´åƒæ•¸
TRAINING_CONFIG = {
    'epochs': 20,
    'batch_size': 16,
    'validation_split': 0.3,
    'learning_rate': 0.001,
    'early_stopping_patience': 5,
    'lr_reduction_patience': 3,
    'dropout_rates': [0.2, 0.3, 0.5]
}

# é€²éšè¨“ç·´åƒæ•¸
ADVANCED_CONFIG = {
    'use_data_augmentation': True,
    'augmentation_factor': 3,
    'use_transfer_learning': False,
    'fine_tune_layers': None,
    'regularization_l2': 0.001
}
```

## ğŸ”„ å¢é‡å­¸ç¿’ç³»çµ±

### æŒçºŒæ”¹é€²æ©Ÿåˆ¶
```python
class IncrementalLearner:
    """å¢é‡å­¸ç¿’ç®¡ç†å™¨"""
    
    def __init__(self, model_manager):
        self.model_manager = model_manager
        self.new_data_buffer = []
        self.min_samples_for_retrain = 50
    
    def add_correction(self, image, true_label, predicted_label):
        """æ·»åŠ äººå·¥ä¿®æ­£çš„æ¨£æœ¬"""
        if predicted_label != true_label:
            self.new_data_buffer.append({
                'image': image,
                'label': true_label,
                'timestamp': datetime.now(),
                'correction_type': 'manual'
            })
    
    def add_low_confidence_sample(self, image, predicted_label, confidence):
        """æ·»åŠ ä½ä¿¡å¿ƒåº¦æ¨£æœ¬"""
        if confidence < 0.8:
            self.new_data_buffer.append({
                'image': image,
                'predicted_label': predicted_label,
                'confidence': confidence,
                'timestamp': datetime.now(),
                'correction_type': 'low_confidence'
            })
    
    def should_retrain(self):
        """åˆ¤æ–·æ˜¯å¦éœ€è¦é‡æ–°è¨“ç·´"""
        return len(self.new_data_buffer) >= self.min_samples_for_retrain
    
    def retrain_model(self):
        """ä½¿ç”¨æ–°è³‡æ–™é‡æ–°è¨“ç·´æ¨¡å‹"""
        if not self.should_retrain():
            return False
        
        # æº–å‚™æ–°è³‡æ–™
        new_images = [sample['image'] for sample in self.new_data_buffer]
        new_labels = [sample['label'] for sample in self.new_data_buffer]
        
        # è¼‰å…¥ç¾æœ‰æ¨¡å‹
        model = self.model_manager.load_model()
        
        # å¢é‡è¨“ç·´
        history = self._incremental_training(model, new_images, new_labels)
        
        # å„²å­˜æ›´æ–°å¾Œçš„æ¨¡å‹
        self.model_manager.save_model(model, backup=True)
        
        # æ¸…ç©ºç·©è¡å€
        self.new_data_buffer.clear()
        
        return True
```

## ğŸ“ˆ æ¨¡å‹æ•ˆèƒ½ç›£æ§

### é æ¸¬å“è³ªè©•ä¼°
```python
class ModelPerformanceMonitor:
    """æ¨¡å‹æ•ˆèƒ½ç›£æ§å™¨"""
    
    def __init__(self):
        self.prediction_history = []
        self.accuracy_window = 100  # æ»‘å‹•è¦–çª—å¤§å°
    
    def record_prediction(self, image, prediction, confidence, is_correct=None):
        """è¨˜éŒ„é æ¸¬çµæœ"""
        record = {
            'timestamp': datetime.now(),
            'prediction': prediction,
            'confidence': confidence,
            'is_correct': is_correct
        }
        self.prediction_history.append(record)
    
    def get_recent_accuracy(self):
        """å–å¾—è¿‘æœŸæº–ç¢ºç‡"""
        recent = self.prediction_history[-self.accuracy_window:]
        correct_predictions = [r for r in recent if r['is_correct'] is True]
        if len(recent) == 0:
            return None
        return len(correct_predictions) / len(recent)
    
    def get_confidence_distribution(self):
        """å–å¾—ä¿¡å¿ƒåº¦åˆ†ä½ˆ"""
        recent = self.prediction_history[-self.accuracy_window:]
        confidences = [r['confidence'] for r in recent]
        return {
            'mean': np.mean(confidences),
            'std': np.std(confidences),
            'min': np.min(confidences),
            'max': np.max(confidences),
            'below_threshold': len([c for c in confidences if c < 0.8])
        }
    
    def should_alert(self):
        """åˆ¤æ–·æ˜¯å¦éœ€è¦è­¦å ±"""
        accuracy = self.get_recent_accuracy()
        if accuracy is not None and accuracy < 0.8:
            return True
        
        conf_dist = self.get_confidence_distribution()
        if conf_dist['below_threshold'] > 20:  # è¶…é 20 å€‹ä½ä¿¡å¿ƒåº¦é æ¸¬
            return True
        
        return False
```

### A/B æ¸¬è©¦æ¡†æ¶
```python
class ModelABTester:
    """æ¨¡å‹ A/B æ¸¬è©¦æ¡†æ¶"""
    
    def __init__(self, model_a, model_b, split_ratio=0.5):
        self.model_a = model_a
        self.model_b = model_b
        self.split_ratio = split_ratio
        self.results_a = []
        self.results_b = []
    
    def predict(self, image):
        """éš¨æ©Ÿé¸æ“‡æ¨¡å‹é€²è¡Œé æ¸¬"""
        if random.random() < self.split_ratio:
            prediction = self.model_a.predict(image)
            self.results_a.append(prediction)
            return prediction, 'model_a'
        else:
            prediction = self.model_b.predict(image)
            self.results_b.append(prediction)
            return prediction, 'model_b'
    
    def get_performance_comparison(self):
        """æ¯”è¼ƒå…©å€‹æ¨¡å‹çš„æ•ˆèƒ½"""
        return {
            'model_a': {
                'count': len(self.results_a),
                'avg_confidence': np.mean([r['confidence'] for r in self.results_a]),
                'accuracy': self._calculate_accuracy(self.results_a)
            },
            'model_b': {
                'count': len(self.results_b),
                'avg_confidence': np.mean([r['confidence'] for r in self.results_b]),
                'accuracy': self._calculate_accuracy(self.results_b)
            }
        }
```

## ğŸ”§ æ¨¡å‹ç®¡ç†ç³»çµ±

### æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶
```python
class ModelVersionManager:
    """æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å™¨"""
    
    def __init__(self, base_path="data/models/"):
        self.base_path = Path(base_path)
        self.current_model = None
        self.model_registry = {}
    
    def save_model(self, model, version=None, metadata=None):
        """å„²å­˜æ¨¡å‹ä¸¦è¨˜éŒ„ç‰ˆæœ¬"""
        if version is None:
            version = self._generate_version()
        
        model_path = self.base_path / f"model_v{version}.keras"
        model.save(model_path)
        
        # è¨˜éŒ„ metadata
        metadata_path = self.base_path / f"model_v{version}_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump({
                'version': version,
                'timestamp': datetime.now().isoformat(),
                'accuracy': metadata.get('accuracy', None),
                'loss': metadata.get('loss', None),
                'architecture': self._get_architecture_info(model),
                'training_config': metadata.get('training_config', {})
            }, f, indent=2)
        
        # æ›´æ–°è¨»å†Šè¡¨
        self.model_registry[version] = {
            'path': model_path,
            'metadata_path': metadata_path,
            'created_at': datetime.now()
        }
        
        return version
    
    def load_model(self, version='latest'):
        """è¼‰å…¥æŒ‡å®šç‰ˆæœ¬çš„æ¨¡å‹"""
        if version == 'latest':
            version = self._get_latest_version()
        
        if version not in self.model_registry:
            raise ValueError(f"Model version {version} not found")
        
        model_path = self.model_registry[version]['path']
        model = tf.keras.models.load_model(model_path)
        self.current_model = model
        
        return model
    
    def list_models(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ç‰ˆæœ¬"""
        return list(self.model_registry.keys())
    
    def rollback_to_version(self, version):
        """å›æ»¾åˆ°æŒ‡å®šç‰ˆæœ¬"""
        model = self.load_model(version)
        
        # å°‡æŒ‡å®šç‰ˆæœ¬è¨­ç‚ºç•¶å‰ç‰ˆæœ¬
        current_path = self.base_path / "model.keras"
        shutil.copy2(self.model_registry[version]['path'], current_path)
        
        return model
```

### æ¨¡å‹éƒ¨ç½²ç®¡é“
```python
class ModelDeploymentPipeline:
    """æ¨¡å‹éƒ¨ç½²ç®¡é“"""
    
    def __init__(self, version_manager, performance_monitor):
        self.version_manager = version_manager
        self.performance_monitor = performance_monitor
        self.deployment_criteria = {
            'min_accuracy': 0.85,
            'min_confidence': 0.8,
            'max_inference_time': 50  # ms
        }
    
    def validate_model(self, model, test_data):
        """é©—è­‰æ¨¡å‹æ˜¯å¦ç¬¦åˆéƒ¨ç½²æ¨™æº–"""
        results = {
            'accuracy': None,
            'avg_confidence': None,
            'inference_time': None,
            'passes_validation': False
        }
        
        # æº–ç¢ºç‡æ¸¬è©¦
        test_loss, test_accuracy = model.evaluate(test_data[0], test_data[1], verbose=0)
        results['accuracy'] = test_accuracy
        
        # ä¿¡å¿ƒåº¦æ¸¬è©¦
        predictions = model.predict(test_data[0])
        confidences = np.max(predictions, axis=1)
        results['avg_confidence'] = np.mean(confidences)
        
        # æ¨ç†æ™‚é–“æ¸¬è©¦
        start_time = time.time()
        _ = model.predict(test_data[0][:10])
        end_time = time.time()
        results['inference_time'] = (end_time - start_time) / 10 * 1000  # ms per sample
        
        # æª¢æŸ¥æ˜¯å¦é€šéæ‰€æœ‰æ¨™æº–
        passes = (
            results['accuracy'] >= self.deployment_criteria['min_accuracy'] and
            results['avg_confidence'] >= self.deployment_criteria['min_confidence'] and
            results['inference_time'] <= self.deployment_criteria['max_inference_time']
        )
        results['passes_validation'] = passes
        
        return results
    
    def deploy_model(self, model, version, test_data):
        """éƒ¨ç½²æ¨¡å‹åˆ°ç”Ÿç”¢ç’°å¢ƒ"""
        # é©—è­‰æ¨¡å‹
        validation_results = self.validate_model(model, test_data)
        
        if not validation_results['passes_validation']:
            raise ValueError(f"Model validation failed: {validation_results}")
        
        # å‚™ä»½ç•¶å‰æ¨¡å‹
        current_model_path = Path("data/models/model.keras")
        if current_model_path.exists():
            backup_path = Path(f"data/models/model_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.keras")
            shutil.copy2(current_model_path, backup_path)
        
        # éƒ¨ç½²æ–°æ¨¡å‹
        model.save(current_model_path)
        
        # è¨˜éŒ„éƒ¨ç½²
        deployment_log = {
            'version': version,
            'timestamp': datetime.now().isoformat(),
            'validation_results': validation_results,
            'deployed_by': 'automated_pipeline'
        }
        
        with open("data/models/deployment_log.json", 'a') as f:
            f.write(json.dumps(deployment_log) + "\n")
        
        return deployment_log
```

## ğŸ¯ æ¨¡å‹æœ€ä½³åŒ–ç­–ç•¥

### é‡åŒ–èˆ‡å£“ç¸®
```python
def optimize_model_for_inference(model):
    """ç‚ºæ¨ç†å„ªåŒ–æ¨¡å‹"""
    
    # TensorFlow Lite è½‰æ›
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    
    # é‡åŒ–è¨­å®š
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_types = [tf.float16]
    
    # è½‰æ›æ¨¡å‹
    tflite_model = converter.convert()
    
    # å„²å­˜å„ªåŒ–å¾Œçš„æ¨¡å‹
    with open('data/models/model_optimized.tflite', 'wb') as f:
        f.write(tflite_model)
    
    return tflite_model

def benchmark_model_performance(model, test_data, num_runs=100):
    """åŸºæº–æ¸¬è©¦æ¨¡å‹æ•ˆèƒ½"""
    import time
    
    # é ç†±
    _ = model.predict(test_data[:1])
    
    # æ¸¬è©¦æ¨ç†æ™‚é–“
    times = []
    for _ in range(num_runs):
        start = time.time()
        _ = model.predict(test_data[:1])
        end = time.time()
        times.append((end - start) * 1000)  # ms
    
    return {
        'avg_inference_time': np.mean(times),
        'std_inference_time': np.std(times),
        'min_inference_time': np.min(times),
        'max_inference_time': np.max(times),
        'p95_inference_time': np.percentile(times, 95)
    }
```

é€éé€™å€‹å®Œæ•´çš„ AI æ¨¡å‹æŒ‡å—ï¼Œæ‚¨å¯ä»¥æœ‰æ•ˆåœ°è¨“ç·´ã€ç®¡ç†å’Œéƒ¨ç½² Make10 å°ˆæ¡ˆä¸­çš„æ•¸å­—è­˜åˆ¥æ¨¡å‹ï¼Œä¸¦å»ºç«‹æŒçºŒæ”¹é€²çš„æ©Ÿåˆ¶ã€‚
